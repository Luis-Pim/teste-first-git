{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI  # Usando o novo cliente 'OpenAI'\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chave = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Índice FAISS e metadados já existem. Pulando criação.\n"
     ]
    }
   ],
   "source": [
    "# Configurar chave de API da OpenAI\n",
    "openai_api_key = chave\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "OCR_PATH = \"tormenta_ocr_result.json\"\n",
    "TABELA_PATH = \"tabelas_tormenta_unificado.json\"\n",
    "INDEX_PATH = \"tormenta_index.faiss\"\n",
    "METADATA_PATH = \"tormenta_metadata.json\"\n",
    "\n",
    "# Modelo de embedding\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# --- ETAPA 1: PRÉ-PROCESSAMENTO ---\n",
    "\n",
    "def carregar_ocr_chunks(path: str) -> List[dict]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ocr = json.load(f)\n",
    "    return [{\"id\": k, \"text\": v} for k, v in ocr.items() if v.strip()]\n",
    "\n",
    "def carregar_tabelas_como_chunks(path: str) -> List[dict]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tabelas = json.load(f)\n",
    "    chunks = []\n",
    "    for tipo, entradas in tabelas.items():\n",
    "        for entrada in entradas:\n",
    "            texto = \"\\n\".join(f\"{k}: {v}\" for k, v in entrada.items())\n",
    "            nome = entrada.get(\"nome\") or entrada.get(\"Nome\") or \"(sem nome)\"\n",
    "            chunks.append({\"id\": f\"{tipo}:{nome}\", \"text\": texto})\n",
    "    return chunks\n",
    "\n",
    "# --- ETAPA 2: EMBEDDINGS ---\n",
    "\n",
    "def gerar_embedding(texto: str) -> List[float]:\n",
    "    # Usando o novo cliente OpenAI para gerar embeddings\n",
    "    response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,  # Modelo para embeddings\n",
    "        input=texto,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    # Acessando o embedding corretamente utilizando atributos do objeto\n",
    "    return response.data[0].embedding  # Acessando diretamente com o atributo 'data' e 'embedding'\n",
    "\n",
    "# --- ETAPA 3: CRIAR BASE FAISS ---\n",
    "\n",
    "def criar_base_faiss(chunks: List[dict]):\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    for chunk in tqdm(chunks, desc=\"Gerando embeddings\"):\n",
    "        emb = gerar_embedding(chunk[\"text\"])\n",
    "        embeddings.append(emb)\n",
    "        metadata.append({\"id\": chunk[\"id\"], \"text\": chunk[\"text\"]})\n",
    "\n",
    "    dim = len(embeddings[0])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "    # Salvar índice e metadados\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "    with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nIndex criado com {len(embeddings)} chunks.\")\n",
    "\n",
    "# --- ETAPA 4: CONSULTA ---\n",
    "\n",
    "def consultar_pergunta(pergunta: str, top_k: int = 5):\n",
    "    pergunta_emb = gerar_embedding(pergunta)\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    D, I = index.search(np.array([pergunta_emb]).astype(\"float32\"), top_k)\n",
    "    contextos = [metadata[i][\"text\"] for i in I[0] if i < len(metadata)]\n",
    "\n",
    "    # Limitação de tokens para baratear o custo\n",
    "    contextos_joined = \" \".join(contextos)\n",
    "    contextos_limitados = contextos_joined[:3500]  # Limita a aproximadamente 3500 caracteres\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Você é um especialista no sistema de RPG Tormenta20. Use os textos abaixo retirados do livro e das regras oficiais para responder à pergunta do jogador.\n",
    "\n",
    "--- Contexto ---\n",
    "{contextos_limitados}\n",
    "\n",
    "--- Pergunta ---\n",
    "{pergunta}\n",
    "\n",
    "--- Resposta ---\n",
    "Responda apenas com base nas informações fornecidas, em português brasileiro.\n",
    "\"\"\"\n",
    "\n",
    "    resposta = client.chat.completions.create(\n",
    "        model=\"gpt-5\", # Ou 'gpt-3.5-turbo'\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Você é um assistente que responde perguntas sobre o RPG Tormenta20.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return resposta.choices[0].message.content\n",
    "\n",
    "# --- CONSULTA POR TEMA ---\n",
    "\n",
    "def consultar_pergunta_por_tema(tema: str):\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    contextos = [item[\"text\"] for item in metadata if tema.lower() in item[\"id\"].lower()]\n",
    "\n",
    "    if not contextos:\n",
    "        return \"Nenhum contexto encontrado sobre esse tema.\"\n",
    "\n",
    "    # Limitação de tokens para baratear o custo\n",
    "    contextos_joined = \" \".join(contextos)\n",
    "    contextos_limitados = contextos_joined[:2000]  # Limita a aproximadamente 3500 caracteres\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Você é um especialista no sistema de RPG Tormenta20.\n",
    "O usuário quer saber sobre o seguinte tema: {tema}.\n",
    "Responda usando as informações detalhadas abaixo.\n",
    "\n",
    "--- Informações ---\n",
    "{contextos_limitados}\n",
    "\n",
    "--- Pergunta ---\n",
    "Liste e explique brevemente os itens relacionados ao tema \\\"{tema}\\\" acima.\n",
    "\n",
    "--- Resposta ---\n",
    "\"\"\"\n",
    "\n",
    "    resposta = client.chat.completions.create(\n",
    "        model=\"gpt-5\",  # Ou 'gpt-3.5-turbo'\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Você é um assistente que responde perguntas sobre o RPG Tormenta20.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return resposta.choices[0].message\n",
    "\n",
    "# --- EXECUÇÃO INICIAL ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not (os.path.exists(INDEX_PATH) and os.path.exists(METADATA_PATH)):\n",
    "        ocr_chunks = carregar_ocr_chunks(OCR_PATH)\n",
    "        tabela_chunks = carregar_tabelas_como_chunks(TABELA_PATH)\n",
    "        todos_chunks = ocr_chunks + tabela_chunks\n",
    "        criar_base_faiss(todos_chunks)\n",
    "    else:\n",
    "        print(\"✅ Índice FAISS e metadados já existem. Pulando criação.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
